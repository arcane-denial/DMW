{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval and Searching by Similarity Part I\n",
    "**Prepared by Christian Alis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb37c61ddfef6c64ca816dee0b7f8b36",
     "grade": false,
     "grade_id": "cell-47a8a5ae78a3d1f2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Tackling the problem of information retrieval is a logical starting point for introducing the issues, paradigms and techniques in solving data analytics problems. The problem of information retrieval can be stated as follows. Given a database or corpus $D$ of documents, find the $k$ most similar documents from a given query document or exemplar $q$. \n",
    "\n",
    "Although the documents that are referred to in the problem are text documents, it can be readily generalized to any object. In this course, we will work with this generalized problem. Thus, when we refer to _information_ retrieval, we actually refer to _object_ retrieval. That is, finding the objects most similar to a query or exemplar object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "875e981081fb120a89286f14fc84a6a8",
     "grade": false,
     "grade_id": "cell-ea98f39fae0c486d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Problem 1**\n",
    "\n",
    "List down three examples of information or object retrieval business problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad95b384d1ec478e4c571893bfea2ea5",
     "grade": true,
     "grade_id": "cell-0f55ad6bb7aeb222",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "1. NBA Draft Pick\n",
    "2. Plagiarism checker\n",
    "3. Recommendations for Google search algo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the information retrieval problem, we need to:\n",
    "\n",
    "1. represent each document or object in a form that is easy to be worked on by algorithms;\n",
    "2. define a measure of similarity between documents or objects;\n",
    "3. return the most similar objects based on the similarity measure.\n",
    "\n",
    "For this lecture, we will discuss further the first two steps. We will only use the simplest algorithm for performing step 3 although there are more efficient algorithms for performing this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector space representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at the problem of how to represent an object. Suppose we want to represent a person, how do we go about this? To answer this, we first need to describe a person i.e., what are the characteristics or features of a person that we can use to define a person? Some of the features that we can use are:\n",
    "\n",
    "* age (continuous)\n",
    "* gender (categorical)\n",
    "* height (continuous)\n",
    "* weight (continuous)\n",
    "\n",
    "As you may have noticed each feature can be considered as a variable in the Statistics sense, so their type, as discussed in ACS, is indicated as well. Even before you started with MSDS, it should be quite intuitive for you to create a spreadsheet with the features above as the columns and each person as a row if you are given a set of people to write down their information for. It might not be obvious to you then but in fact, a spreadsheet of features as columns and objects as rows _is_ a matrix and each row _is_ a vector. The matrix and row that we are referring to are the same _matrix_ and _row_ that were discussed in MDS.\n",
    "\n",
    "You may be wondering: but what about gender--it's not a number! Indeed you are correct. To complete our _vector space representation_ of people, we have to convert age, which is a categorical variable, into numbers. The usual approach is to perform one-hot encoding. This is the same as creating dummy or indicator variables in Statistics.\n",
    "\n",
    "To one-hot encode a categorical variable with $N$ unique values, we replace the variable with $N$ features with each feature corresponding to a unique value of the variable. The new features are indicator variables: we assign a value of 1 to the corresponding indicator variable and we set a value of 0 to the other new features. If the number of unique values is quite small, the new features won't be totally independent. To avoid this, we can omit creating a new feature for one of the feature values, effectively assigning the all-zero value to the omitted feature value. If a categorical value has only two unique values then we can replace it with just one indicator variable, effectively converting the original categorical feature into a boolean.\n",
    "\n",
    "The resulting matrix is known as a _design matrix_. The columns correspond to features while the rows correspond to objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cd9372c37d8f5579026b8870abf62ce",
     "grade": false,
     "grade_id": "cell-cf732850ec1ab36d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Problem 2**\n",
    "\n",
    "Create a function `onehot_encode` that accepts a list of categorical values and returns their one-hot encoded values as a pandas DataFrame. If there is only one unique value, convert it to an indicator by dropping the first sorted value. Sort the columns based on their corresponding categorical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T03:10:08.996818Z",
     "start_time": "2020-06-14T03:09:29.779Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "718fc46bf16587d43cee04fcb618f2d6",
     "grade": false,
     "grade_id": "cell-4ccb1a6db4b2b0bd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from numpy.testing import (assert_equal, assert_almost_equal,\n",
    "                           assert_array_equal, assert_array_almost_equal)\n",
    "from pandas.testing import assert_frame_equal\n",
    "from IPython.display import display, display_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:29.935948Z",
     "start_time": "2020-06-12T13:58:29.930532Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "967e2ee526bd5f96609235f269816be8",
     "grade": false,
     "grade_id": "cell-7d01db790cd6b032",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def onehot_encode(cat_values):\n",
    "    '''\n",
    "    Create a function that accepts a list of categorical variable and return \n",
    "    their one- hot encoded values as DF\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    series = pd.Series(cat_values)\n",
    "    u = series.unique()\n",
    "    res = None\n",
    "    if len(u) >= 3:\n",
    "        res = pd.get_dummies(series)\n",
    "    else:\n",
    "        res = pd.get_dummies(series, drop_first=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:29.967226Z",
     "start_time": "2020-06-12T13:58:29.939004Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d321c5f37b97cdcbe8c78e1c9adf2d9e",
     "grade": true,
     "grade_id": "cell-eeb396dd1b8c0991",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_frame_equal(\n",
    "    onehot_encode(['a', 'a', 'b', 'c', 'a', 'd', 'e']),\n",
    "    pd.DataFrame(\n",
    "        {'a': {0: 1, 1: 1, 2: 0, 3: 0, 4: 1, 5: 0, 6: 0},\n",
    "         'b': {0: 0, 1: 0, 2: 1, 3: 0, 4: 0, 5: 0, 6: 0},\n",
    "         'c': {0: 0, 1: 0, 2: 0, 3: 1, 4: 0, 5: 0, 6: 0},\n",
    "         'd': {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 0},\n",
    "         'e': {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1}}),\n",
    "    check_dtype=False)\n",
    "assert_frame_equal(\n",
    "    onehot_encode(['m', 'm', 'f', 'f']).astype(int),\n",
    "    pd.DataFrame({'m': {0: 1, 1: 1, 2: 0, 3: 0}}))\n",
    "assert_frame_equal(\n",
    "    onehot_encode(['f', 'm', 'm', 'f', 'f']).astype(int),\n",
    "    pd.DataFrame({'m': {0: 0, 1: 1, 2: 1, 3: 0, 4: 0}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "632c2942a3214c7bdd76101ebfbc7985",
     "grade": false,
     "grade_id": "cell-4e134caf04052d21",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Problem 3**\n",
    "\n",
    "Create a function `vectorize_people` that accepts a list of people in the form of `dict`s and returns the corresponding design matrix as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:29.976068Z",
     "start_time": "2020-06-12T13:58:29.968744Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "529b656679a74ae1eafc85857bbecf61",
     "grade": false,
     "grade_id": "cell-03c8d6cf693785ad",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_people(people):\n",
    "    \"\"\"\n",
    "    Returns the corresponding design matrix as pandas DataFrame from the\n",
    "    given list of people in the form of dicts\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(pd.DataFrame(people), \n",
    "                          drop_first=True).sort_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:29.999626Z",
     "start_time": "2020-06-12T13:58:29.980513Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "073fbcc12dd4d32428c5e72e164946d2",
     "grade": true,
     "grade_id": "cell-dee77df3e42da014",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_frame_equal(\n",
    "    vectorize_people([\n",
    "            {'age': 20, 'gender': 'f', 'height': 1.2, 'weight': 50},\n",
    "            {'age': 23, 'gender': 'm', 'height': 1.4, 'weight': 62},\n",
    "            {'age': 42, 'gender': 'f', 'height': 1.5, 'weight': 58},\n",
    "            {'age': 31, 'gender': 'f', 'height': 1.3, 'weight': 56},\n",
    "            {'age': 28, 'gender': 'm', 'height': 1.6, 'weight': 70}]),\n",
    "    pd.DataFrame(\n",
    "        {'age': {0: 20, 1: 23, 2: 42, 3: 31, 4: 28},\n",
    "         'gender_m': {0: 0, 1: 1, 2: 0, 3: 0, 4: 1},\n",
    "         'height': {0: 1.2, 1: 1.4, 2: 1.5, 3: 1.3, 4: 1.6},\n",
    "         'weight': {0: 50, 1: 62, 2: 58, 3: 56, 4: 70}}),\n",
    "    check_dtype=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, let us consider wines. We can describe wines based on their chemical composition. The [UCI ML Wine recognition datasets](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data) is a publicly available dataset that we can work with. It can conveniently be accessed using `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:30.326341Z",
     "start_time": "2020-06-12T13:58:30.002962Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "data_wine = load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `load_wine` returns a `Bunch` object which is basically a dictionary of both data and metadata. Here are the items of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:30.333511Z",
     "start_time": "2020-06-12T13:58:30.327858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wine.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the description of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T03:12:03.297609Z",
     "start_time": "2020-06-14T03:12:03.290463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_wine['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:30.357552Z",
     "start_time": "2020-06-12T13:58:30.348337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "        1.065e+03],\n",
       "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "        1.050e+03],\n",
       "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "        1.185e+03],\n",
       "       ...,\n",
       "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "        8.350e+02],\n",
       "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "        8.400e+02],\n",
       "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "        5.600e+02]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wine['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's more convenient to convert the data into a pandas DataFrame with column names corresponding to the feature. I will leave that to you as an (ungraded) exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09912c9b113ad74c99b03178f28f1add",
     "grade": false,
     "grade_id": "cell-6898b7b0229f49d9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Problem 4**\n",
    "\n",
    "Create a function `get_wine_df` that accepts a wine `Bunch` object and returns the design matrix as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:30.365462Z",
     "start_time": "2020-06-12T13:58:30.360923Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc5dfb2c7e67a166e2f79475615cb992",
     "grade": false,
     "grade_id": "cell-caf8cd502b90d28f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_wine_df(data_wine):\n",
    "     return pd.DataFrame(data_wine['data'], columns=data_wine['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:30.380782Z",
     "start_time": "2020-06-12T13:58:30.368500Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac624e8b6c7e83c51ff6e8e53871526e",
     "grade": true,
     "grade_id": "cell-71a537e6e2a09166",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_wine = get_wine_df(data_wine)\n",
    "assert_equal(type(df_wine), pd.DataFrame)\n",
    "assert_array_equal(\n",
    "    df_wine.columns,\n",
    "    ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
    "     'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
    "     'proanthocyanins', 'color_intensity', 'hue',\n",
    "     'od280/od315_of_diluted_wines', 'proline']\n",
    ")\n",
    "assert_array_almost_equal(\n",
    "    df_wine.iloc[0],\n",
    "    [1.423e+01, 1.710e+00, 2.430e+00, 1.560e+01, 1.270e+02, 2.800e+00,\n",
    "     3.060e+00, 2.800e-01, 2.290e+00, 5.640e+00, 1.040e+00, 3.920e+00,\n",
    "     1.065e+03]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to represent objects in general, let us look at how to represent text. Since vectors are convenient for representing objects, we also want to represent texts as vectors. But what should be our features? One approach is to look at stylometrics: how long the sentences are, how frequent is each part of speech and so on. This works but if we want to represent the actual text itself, one of the most common ways of doing this is by using its bag-of-words (BoW) representation.\n",
    "\n",
    "The bag-of-words vector of a text is a vector where each component corresponds to a unique word (token or term) and the value corresponds to the number of occurrences of that word in the text. This is basically the count of each word in the text. Doing this removes the order of the words and their context. However, for many applications, the bag-of-words representation suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c8b2af0eaf1d2f46b68e51871491fd8",
     "grade": false,
     "grade_id": "cell-591bffb7dac1ad66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Problem 5**\n",
    "\n",
    "Create a function `to_bow` that accepts a list of documents and returns a pandas DataFrame of their bag-of-words representation. Sort the columns alphabetically. Assume that a word or token is any consecutive non-whitespace substring. Make them case-insensitive by converting to lowercase. **Do not** use `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:30.395616Z",
     "start_time": "2020-06-12T13:58:30.382871Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dffdc8e4a06daca5b1e5197171dc298",
     "grade": false,
     "grade_id": "cell-d8f9ae3c19674cc5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def to_bow(docs):\n",
    "    \"\"\"\n",
    "    Returns pandas DataFrame of bag-of-words representation from a list of \n",
    "    documents. Words or Token is any consecutive non-whitespace subsetring.\n",
    "    \"\"\"\n",
    "    docs = [dict(Counter(doc.lower().split())) for doc in docs]\n",
    "    return pd.DataFrame(docs).fillna(0).sort_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:58:30.415273Z",
     "start_time": "2020-06-12T13:58:30.397617Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db517aeaa90bc7725c1d290a47b9fea4",
     "grade": true,
     "grade_id": "cell-2feeb16ba3c459e6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_frame_equal(\n",
    "    to_bow(['mary had a little lamb', 'little lamb', \n",
    "            'little lamb', 'as white as snow']),\n",
    "    pd.DataFrame(\n",
    "        {'a': {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
    "         'as': {0: 0.0, 1: 0.0, 2: 0.0, 3: 2.0},\n",
    "         'had': {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
    "         'lamb': {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.0},\n",
    "         'little': {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.0},\n",
    "         'mary': {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
    "         'snow': {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0},\n",
    "         'white': {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0}}),\n",
    "    check_dtype=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let us work with the 20 newsgroups dataset which consists of 18000 posts from 20 different [newsgroups](https://en.wikipedia.org/wiki/Usenet_newsgroup). For simplicity, we will only consider posts in the `comp.graphics` and `rec.autos` newsgroups.\n",
    "\n",
    "We load the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T14:01:16.943987Z",
     "start_time": "2020-06-12T13:58:30.417343Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data_newsgroups = fetch_20newsgroups(\n",
    "    subset='all', \n",
    "    categories=['comp.graphics', 'rec.autos'],\n",
    "    shuffle=False, \n",
    "    remove=['headers', 'footers', 'quotes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a description of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T14:01:16.949715Z",
     "start_time": "2020-06-12T14:01:16.946221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_newsgroups['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert the data into their BoW representation defining a word as a sequence of non-whitespace characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:57.241034Z",
     "start_time": "2020-06-14T02:05:55.446619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>!!</th>\n",
       "      <th>!!!</th>\n",
       "      <th>!!!!</th>\n",
       "      <th>!!!!!!!!!!</th>\n",
       "      <th>!)</th>\n",
       "      <th>!:</th>\n",
       "      <th>!=</th>\n",
       "      <th>!&gt;</th>\n",
       "      <th>!changefsi,</th>\n",
       "      <th>...</th>\n",
       "      <th>~ftp/volvis92</th>\n",
       "      <th>~re00/tmp/sm.2.1.0.tar.z</th>\n",
       "      <th>~~15</th>\n",
       "      <th>~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>Ã¾</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1892 rows Ã 39080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        !   !!  !!!  !!!!  !!!!!!!!!!   !)   !:   !=   !>  !changefsi,  ...  \\\n",
       "0     0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "1     0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "2     0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "3     0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "4     0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "...   ...  ...  ...   ...         ...  ...  ...  ...  ...          ...  ...   \n",
       "1887  0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "1888  0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "1889  0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "1890  0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "1891  0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "\n",
       "      ~ftp/volvis92  ~re00/tmp/sm.2.1.0.tar.z  ~~15  ~~~~~~~~~~~  \\\n",
       "0               0.0                       0.0   0.0          0.0   \n",
       "1               0.0                       0.0   0.0          0.0   \n",
       "2               0.0                       0.0   0.0          0.0   \n",
       "3               0.0                       0.0   0.0          0.0   \n",
       "4               0.0                       0.0   0.0          0.0   \n",
       "...             ...                       ...   ...          ...   \n",
       "1887            0.0                       0.0   0.0          0.0   \n",
       "1888            0.0                       0.0   0.0          0.0   \n",
       "1889            0.0                       0.0   0.0          0.0   \n",
       "1890            0.0                       0.0   0.0          0.0   \n",
       "1891            0.0                       0.0   0.0          0.0   \n",
       "\n",
       "      ~~~~~~~~~~~~  ~~~~~~~~~~~~~~~~~~~  \\\n",
       "0              0.0                  0.0   \n",
       "1              0.0                  0.0   \n",
       "2              0.0                  0.0   \n",
       "3              0.0                  0.0   \n",
       "4              0.0                  0.0   \n",
       "...            ...                  ...   \n",
       "1887           0.0                  0.0   \n",
       "1888           0.0                  0.0   \n",
       "1889           0.0                  0.0   \n",
       "1890           0.0                  0.0   \n",
       "1891           0.0                  0.0   \n",
       "\n",
       "      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "0                                                   0.0         \n",
       "1                                                   0.0         \n",
       "2                                                   0.0         \n",
       "3                                                   0.0         \n",
       "4                                                   0.0         \n",
       "...                                                 ...         \n",
       "1887                                                0.0         \n",
       "1888                                                0.0         \n",
       "1889                                                0.0         \n",
       "1890                                                0.0         \n",
       "1891                                                0.0         \n",
       "\n",
       "      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "0                                                   0.0                         \n",
       "1                                                   0.0                         \n",
       "2                                                   0.0                         \n",
       "3                                                   0.0                         \n",
       "4                                                   0.0                         \n",
       "...                                                 ...                         \n",
       "1887                                                0.0                         \n",
       "1888                                                0.0                         \n",
       "1889                                                0.0                         \n",
       "1890                                                0.0                         \n",
       "1891                                                0.0                         \n",
       "\n",
       "      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "0                                                   0.0                               \n",
       "1                                                   0.0                               \n",
       "2                                                   0.0                               \n",
       "3                                                   0.0                               \n",
       "4                                                   0.0                               \n",
       "...                                                 ...                               \n",
       "1887                                                0.0                               \n",
       "1888                                                0.0                               \n",
       "1889                                                0.0                               \n",
       "1890                                                0.0                               \n",
       "1891                                                0.0                               \n",
       "\n",
       "        Ã¾  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "...   ...  \n",
       "1887  0.0  \n",
       "1888  0.0  \n",
       "1889  0.0  \n",
       "1890  0.0  \n",
       "1891  0.0  \n",
       "\n",
       "[1892 rows x 39080 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_ng = to_bow(list(filter(lambda post: post.strip(), \n",
    "                            data_newsgroups['data'])))\n",
    "bow_ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, many of the \"words\" are not \"words\" in the intuitive sense. Natural language processing usually involves a lot of preprocessing to get to the desired performance. For now, we leave them as is and put off any attempts at improving them until later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have converted our objects and documents into their vector space representation, we now have to define a measure of how similar or different two objects or documents are. Recall that vectors can be thought of as arrows with its tip pointing to where that point is in space. We can then use the distance of those points or the angle between the arrows as measures of similarity between the objects or documents that they represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L_p$-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_p$-norm is basically the distance between two vectors or points. The $L_p$-norm between two vectors $\\vec v_1$ and $\\vec v_2$ is\n",
    "$$L_p(\\vec v_1, \\vec v_2) = \\left(\\sum_i \\left| \\vec v_{1_i} - \\vec v_{2_i} \\right|^p \\right)^{1/p}$$\n",
    "\n",
    "If $p=2$, it is the usual Euclidean distance. If $p=1$, it is known as the city block or Manhattan distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51383abb27a737f0896c32dbb65c2090",
     "grade": false,
     "grade_id": "cell-cf0d8a2f981427b8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Problem 6**\n",
    "\n",
    "Create a function `lpnorm` that accepts two vectors in the form of an `ndarray` and an optional value for `p` (default: 2) and returns the $L_p$-norm of the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:57.249258Z",
     "start_time": "2020-06-14T02:05:57.243502Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c8b8bb05bd58afc0c97df416540df1d",
     "grade": false,
     "grade_id": "cell-70271098ae47a4f3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def lpnorm(vec1, vec2, p=2):\n",
    "    \"\"\"Compute the L_p-norm distance between vec1 and vec2\n",
    "    \n",
    "    If `vec1` and `vec2` are same-sized matrices, an ndarray of the L_p-norm \n",
    "    of corresponding rows will be returned instead.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : ndarray\n",
    "        First vector\n",
    "    vec2 : ndarray\n",
    "        Second vector\n",
    "    p : int or float, optional\n",
    "        Order of L_p norm; the `p` in L_p norm\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        L_p norm distance of `vec1` and `vec2`\n",
    "    \"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    return (np.abs(vec1 - vec2)**p).sum(-1)**(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:57.261958Z",
     "start_time": "2020-06-14T02:05:57.250747Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67cd81e43eb527b4879d64646af11dd4",
     "grade": true,
     "grade_id": "cell-d081ba12727bbe8b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_almost_equal(\n",
    "    lpnorm(np.array([1, 2, 3]), np.array([1, 2, 3])), \n",
    "    0\n",
    ")\n",
    "assert_almost_equal(\n",
    "    lpnorm(np.array([1, 2, 3]), np.array([3, 2, 1])), \n",
    "    2.8284271247461903\n",
    ")\n",
    "assert_almost_equal(\n",
    "    lpnorm(np.array([1, 2, 3]), np.array([3, 2, 1]), p=1), \n",
    "    4.0\n",
    ")\n",
    "assert_almost_equal(\n",
    "    lpnorm(np.array([[1, 2, 3],\n",
    "                     [3, 2, 1]]), \n",
    "           np.array([[3, 2, 1],\n",
    "                    [2, 3, 1]])), \n",
    "    [2.82842712, 1.41421356]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T03:12:26.409241Z",
     "start_time": "2020-06-14T03:12:26.384282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between the first two wines is 31.27.\n",
      "The Manhattan distance between the first two wines is 51.06.\n",
      "The Euclidean distance between the first two posts is 65.00.\n",
      "The Manhattan distance between the first two posts is 65.00.\n"
     ]
    }
   ],
   "source": [
    "print('The Euclidean distance between the first two wines is %0.2f.' % \n",
    "      lpnorm(df_wine.iloc[0,:], df_wine.iloc[1, :]))\n",
    "print('The Manhattan distance between the first two wines is {:0.2f}.'.format(\n",
    "      lpnorm(df_wine.iloc[0,:], df_wine.iloc[1, :], 1)))\n",
    "print('The Euclidean distance between the first two posts is %0.2f.' % \n",
    "      lpnorm(bow_ng.iloc[0,:], bow_ng.iloc[1, :]))\n",
    "print('The Manhattan distance between the first two posts is {:0.2f}.'.format(\n",
    "      lpnorm(bow_ng.iloc[0,:], bow_ng.iloc[1, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do we select a value for `p`? Consider a unit cube of dimensionality $d=10$ fully located in the nonnegative quadrant with one corner at the origin. We then take the $L_p$-norm distance of random points in the cube to the origin. We define contrast as the ratio of the variation of the distances to the mean distance. The plot of the contrast for different values of `p` is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:58.170381Z",
     "start_time": "2020-06-14T02:05:57.286582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "contrasts = []\n",
    "ps = np.arange(1, 11)\n",
    "for p in ps:\n",
    "    distances = lpnorm(np.zeros((100000, 10)), \n",
    "                       np.random.random((100000, 10)), \n",
    "                       p)\n",
    "    # (mu + 3*sigma) - (mu - 3*sigma)\n",
    "    contrasts.append(6*distances.std() / distances.mean())\n",
    "ax.plot(ps, contrasts, '-o')\n",
    "ax.set_xlabel('$p$')\n",
    "ax.set_ylabel('Contrast');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity between two vectors is related to the angle between them. The cosine similarity between two vectors $\\vec v_1$ and $\\vec v_2$ is\n",
    "$$S_\\text{cos}(\\vec v_1, \\vec v_2) = \\frac{\\vec v_1 \\cdot \\vec v_2}{\\left|\\left|\\vec v_1\\right|\\right| \\left|\\left| \\vec v_2 \\right|\\right|}.$$\n",
    "\n",
    "Its range is $[0,1]$ with 0 implying the two vectors are aligned (most similar) and 1 implying they are perpendicular (least similar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bdbfd55714ce797f594b46dac6fcf72",
     "grade": false,
     "grade_id": "cell-7fbf7130a8eeaa95",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Problem 7**\n",
    "\n",
    "Create a function `cossim` that accepts two vectors in the form of an `ndarray` and returns their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:58.176141Z",
     "start_time": "2020-06-14T02:05:58.172315Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52aa1180abbcfacafa0270510f17072c",
     "grade": false,
     "grade_id": "cell-b5fa9cf7398f5a16",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cossim(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between vec1 and vec2\n",
    "    \n",
    "    If `vec1` and `vec2` are same-sized matrices, an ndarray of the cosine \n",
    "    similarity of corresponding rows will be returned instead.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : ndarray\n",
    "        First vector\n",
    "    vec2 : ndarray\n",
    "        Second vector\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        cosine similarity of `vec1` and `vec2`\n",
    "    \"\"\"\n",
    "    v1, v2 = (np.array(vec1), np.array(vec2))\n",
    "\n",
    "    es = np.einsum\n",
    "    op = 'ij,ij->i'\n",
    "\n",
    "    if len(v1.shape) > 1 or len(v2.shape) > 1:    \n",
    "        numerator = es(op, v1, v2)\n",
    "    else:\n",
    "        numerator = v1.dot(v2)\n",
    "\n",
    "    return numerator/(np.sqrt(((v1)**2).sum(-1))*np.sqrt(((v2)**2).sum(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:58.184900Z",
     "start_time": "2020-06-14T02:05:58.177500Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2651ff3d0a663654b782e62d25c3c200",
     "grade": true,
     "grade_id": "cell-1c3380ccaa4cc232",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_almost_equal(\n",
    "    cossim(np.array([1, 2, 3]), np.array([1, 2, 3])), \n",
    "    1.0\n",
    ")\n",
    "assert_almost_equal(\n",
    "    cossim(np.array([1, 2, 3]), np.array([3, 2, 1])), \n",
    "    0.7142857142857143\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:58.222984Z",
     "start_time": "2020-06-14T02:05:58.186711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the first two wines is 1.00.\n",
      "The cosine similarity between the first two posts is 0.39.\n"
     ]
    }
   ],
   "source": [
    "print('The cosine similarity between the first two wines is %0.2f.' % \n",
    "      cossim(df_wine.iloc[0,:], df_wine.iloc[1, :]))\n",
    "print('The cosine similarity between the first two posts is {:0.2f}.'.format(\n",
    "      cossim(bow_ng.iloc[0,:], bow_ng.iloc[1, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of cosine similarity is opposite to what we consider as distance. To convert cosine similarity to cosine distance, we subtract the value of the cosine similarity from 1:\n",
    "$$d_\\text{cos}(\\vec v_1, \\vec v_2) = 1 - S_\\text{cos}(\\vec v_1, \\vec v_2) = 1 - \\frac{\\vec v_1 \\cdot \\vec v_2}{\\left|\\left|\\vec v_1\\right|\\right| \\left|\\left| \\vec v_2 \\right|\\right|}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d5a87c6c7a98a97eb1f18829d8e39ae",
     "grade": false,
     "grade_id": "cell-37b92fb1a9f5c460",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Problem 8**\n",
    "\n",
    "Create a function `dcos` that accepts two vectors in the form of an `ndarray` and returns their cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:58.320088Z",
     "start_time": "2020-06-14T02:05:58.225606Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35b0471a35f5c4f0fe7bcc5295b6f3d6",
     "grade": false,
     "grade_id": "cell-253ae14065266712",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dcos(vec1, vec2):\n",
    "    \"\"\"Compute cosine distance between vec1 and vec2\n",
    "    \n",
    "    If `vec1` and `vec2` are same-sized matrices, an ndarray of the cosine \n",
    "    distance of corresponding rows will be returned instead.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : ndarray\n",
    "        First vector\n",
    "    vec2 : ndarray\n",
    "        Second vector\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        cosine distance of `vec1` and `vec2`\n",
    "    \"\"\"\n",
    "    return 1 - cossim(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:58.422664Z",
     "start_time": "2020-06-14T02:05:58.325328Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "386c395802a05fda569abb555023b8d1",
     "grade": true,
     "grade_id": "cell-ab5bca97239debd7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_almost_equal(dcos(np.array([1, 2, 3]), \n",
    "                                      np.array([1, 2, 3])), \n",
    "                               0.0)\n",
    "assert_almost_equal(dcos(np.array([1, 2, 3]), \n",
    "                                      np.array([3, 2, 1])), \n",
    "                               0.2857142857142857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:58.518789Z",
     "start_time": "2020-06-14T02:05:58.426425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine distance between the first two wines is 0.00.\n",
      "The cosine distance between the first two posts is 0.61.\n"
     ]
    }
   ],
   "source": [
    "print('The cosine distance between the first two wines is %0.2f.' % \n",
    "      dcos(df_wine.iloc[0,:], df_wine.iloc[1, :]))\n",
    "print('The cosine distance between the first two posts is {:0.2f}.'.format(\n",
    "      dcos(bow_ng.iloc[0,:], bow_ng.iloc[1, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have represented our objects and documents as vectors and also defined measures for similarity that we can use, we can now look for similar objects or documents of a given exemplar object or document. The algorithm for doing so is quite straightforward: \n",
    "\n",
    "1. transform the exemplar into the same representation as the objects in the database\n",
    "2. compute the similarity or distance of the exemplar with each of the objects in the database\n",
    "3. return the objects in the database with the greatest similarity or smallest distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25204d95efe614382e4fc6dd8121ba0a",
     "grade": false,
     "grade_id": "cell-810a81dae4e3b907",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Problem 9**\n",
    "\n",
    "Create a function `nearest_k` that accepts a vector-represented query `query`, a database of vector-represented objects `objects`, the number of objects `k` to return, and the distance function `dist` and returns the indices of the `k` most similar objects, sorted by decreasing similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:58.619526Z",
     "start_time": "2020-06-14T02:05:58.523721Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3007fec5c71dee4bb2f265ec1bb81d7",
     "grade": false,
     "grade_id": "cell-a53cb3009a933d5b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def nearest_k(query, objects, k, dist):\n",
    "    \"\"\"Return the indices to objects most similar to query\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query : ndarray\n",
    "        query object represented in the same form vector representation as the\n",
    "        objects\n",
    "    objects : ndarray\n",
    "        vector-represented objects in the database; rows correspond to \n",
    "        objects, columns correspond to features\n",
    "    k : int\n",
    "        number of most similar objects to return\n",
    "    dist : function\n",
    "        accepts two ndarrays as parameters then returns their distance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Indices to the most similar objects in the database\n",
    "    \"\"\"\n",
    "    query_matrix = np.vstack([query]*len(objects))\n",
    "    \n",
    "    dist_res = dist(query_matrix, objects)\n",
    "\n",
    "    return np.argpartition(dist_res, k)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:05:58.722338Z",
     "start_time": "2020-06-14T02:05:58.624256Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6973b7c66b2ee5769e09bd7e905b87b6",
     "grade": true,
     "grade_id": "cell-15c5a9734250c18b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "objects = np.array([[5, 6, 7],\n",
    "                    [8, 9, 10],\n",
    "                    [3, 2, 1],\n",
    "                    [1, 2, 3],\n",
    "                    [4, 5, 6]])\n",
    "assert_equal(\n",
    "    nearest_k(np.array([1, 2, 3]), objects, 1, lpnorm), \n",
    "    np.array([3])\n",
    ")\n",
    "assert_equal(\n",
    "    nearest_k(np.array([1, 2, 3]), objects, 2, lpnorm),\n",
    "    np.array([3, 2])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `nearest_k`, we can perform some sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T03:13:04.918704Z",
     "start_time": "2020-06-14T03:13:04.791268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>The features of the first wine are:</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "alcohol                           14.23\n",
       "malic_acid                         1.71\n",
       "ash                                2.43\n",
       "alcalinity_of_ash                 15.60\n",
       "magnesium                        127.00\n",
       "total_phenols                      2.80\n",
       "flavanoids                         3.06\n",
       "nonflavanoid_phenols               0.28\n",
       "proanthocyanins                    2.29\n",
       "color_intensity                    5.64\n",
       "hue                                1.04\n",
       "od280/od315_of_diluted_wines       3.92\n",
       "proline                         1065.00\n",
       "Name: 0, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>The features of the 5 wines most similar to the first wine are:</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>13.74</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.25</td>\n",
       "      <td>16.4</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.62</td>\n",
       "      <td>5.85</td>\n",
       "      <td>0.92</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>14.21</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.44</td>\n",
       "      <td>18.9</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14.10</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.40</td>\n",
       "      <td>18.8</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.38</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>14.38</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.19</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0     14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "54    13.74        1.67  2.25               16.4      118.0           2.60   \n",
       "45    14.21        4.04  2.44               18.9      111.0           2.85   \n",
       "48    14.10        2.02  2.40               18.8      103.0           2.75   \n",
       "46    14.38        3.59  2.28               16.0      102.0           3.25   \n",
       "\n",
       "    flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0         3.06                  0.28             2.29             5.64  1.04   \n",
       "54        2.90                  0.21             1.62             5.85  0.92   \n",
       "45        2.65                  0.30             1.25             5.24  0.87   \n",
       "48        2.92                  0.32             2.38             6.20  1.07   \n",
       "46        3.17                  0.27             2.19             4.90  1.04   \n",
       "\n",
       "    od280/od315_of_diluted_wines  proline  \n",
       "0                           3.92   1065.0  \n",
       "54                          3.20   1060.0  \n",
       "45                          3.33   1080.0  \n",
       "48                          2.75   1060.0  \n",
       "46                          3.44   1065.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_html('<b>The features of the first wine are:</b>', raw=True)\n",
    "display(df_wine.iloc[0])\n",
    "display_html('<b>The features of the 5 wines most similar to the first wine '\n",
    "             'are:</b>', raw=True)\n",
    "display(df_wine.iloc[\n",
    "    nearest_k(df_wine.iloc[0], \n",
    "              df_wine.to_numpy(), \n",
    "              5, \n",
    "              lpnorm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T03:11:09.858111Z",
     "start_time": "2020-06-14T03:10:22.709877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>The features of the first post are:</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "!                                                                                0.0\n",
       "!!                                                                               0.0\n",
       "!!!                                                                              0.0\n",
       "!!!!                                                                             0.0\n",
       "!!!!!!!!!!                                                                       0.0\n",
       "                                                                                ... \n",
       "~~~~~~~~~~~~~~~~~~~                                                              0.0\n",
       "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                          0.0\n",
       "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~          0.0\n",
       "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    0.0\n",
       "Ã¾                                                                                0.0\n",
       "Name: 0, Length: 39080, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>The features of the 5 posts most similar to the first post are:</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>!!</th>\n",
       "      <th>!!!</th>\n",
       "      <th>!!!!</th>\n",
       "      <th>!!!!!!!!!!</th>\n",
       "      <th>!)</th>\n",
       "      <th>!:</th>\n",
       "      <th>!=</th>\n",
       "      <th>!&gt;</th>\n",
       "      <th>!changefsi,</th>\n",
       "      <th>...</th>\n",
       "      <th>~ftp/volvis92</th>\n",
       "      <th>~re00/tmp/sm.2.1.0.tar.z</th>\n",
       "      <th>~~15</th>\n",
       "      <th>~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>Ã¾</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 39080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        !   !!  !!!  !!!!  !!!!!!!!!!   !)   !:   !=   !>  !changefsi,  ...  \\\n",
       "463   0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "0     0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "1823  0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "850   0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "858   0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "\n",
       "      ~ftp/volvis92  ~re00/tmp/sm.2.1.0.tar.z  ~~15  ~~~~~~~~~~~  \\\n",
       "463             0.0                       0.0   0.0          0.0   \n",
       "0               0.0                       0.0   0.0          0.0   \n",
       "1823            1.0                       0.0   0.0          0.0   \n",
       "850             0.0                       0.0   0.0          0.0   \n",
       "858             1.0                       0.0   0.0          0.0   \n",
       "\n",
       "      ~~~~~~~~~~~~  ~~~~~~~~~~~~~~~~~~~  \\\n",
       "463            0.0                  0.0   \n",
       "0              0.0                  0.0   \n",
       "1823           0.0                  0.0   \n",
       "850            0.0                  0.0   \n",
       "858            0.0                  0.0   \n",
       "\n",
       "      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "463                                                 0.0         \n",
       "0                                                   0.0         \n",
       "1823                                                0.0         \n",
       "850                                                 0.0         \n",
       "858                                                 0.0         \n",
       "\n",
       "      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "463                                                 0.0                         \n",
       "0                                                   0.0                         \n",
       "1823                                                0.0                         \n",
       "850                                                 0.0                         \n",
       "858                                                 0.0                         \n",
       "\n",
       "      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "463                                                 0.0                               \n",
       "0                                                   0.0                               \n",
       "1823                                                0.0                               \n",
       "850                                                 0.0                               \n",
       "858                                                 0.0                               \n",
       "\n",
       "        Ã¾  \n",
       "463   0.0  \n",
       "0     0.0  \n",
       "1823  0.0  \n",
       "850   0.0  \n",
       "858   0.0  \n",
       "\n",
       "[5 rows x 39080 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_html('<b>The features of the first post are:</b>', raw=True)\n",
    "display(bow_ng.iloc[0])\n",
    "display_html('<b>The features of the 5 posts most similar to the first post '\n",
    "             'are:</b>', raw=True)\n",
    "display(bow_ng.iloc[nearest_k(bow_ng.iloc[0], bow_ng.to_numpy(), 5, dcos)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the most similar object is the first object in the database--it's the query after all. Let's try searching for something that is not in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T03:08:36.862450Z",
     "start_time": "2020-06-14T03:08:36.820190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14.10</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.96</td>\n",
       "      <td>7.50</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.19</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.48</td>\n",
       "      <td>16.5</td>\n",
       "      <td>108.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.86</td>\n",
       "      <td>8.70</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>13.58</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.36</td>\n",
       "      <td>19.1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.95</td>\n",
       "      <td>6.90</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1515.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "10    14.10        2.16  2.30               18.0      105.0           2.95   \n",
       "14    14.38        1.87  2.38               12.0      102.0           3.30   \n",
       "18    14.19        1.59  2.48               16.5      108.0           3.30   \n",
       "3     14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "31    13.58        1.66  2.36               19.1      106.0           2.86   \n",
       "\n",
       "    flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "10        3.32                  0.22             2.38             5.75  1.25   \n",
       "14        3.64                  0.29             2.96             7.50  1.20   \n",
       "18        3.93                  0.32             1.86             8.70  1.23   \n",
       "3         3.49                  0.24             2.18             7.80  0.86   \n",
       "31        3.19                  0.22             1.95             6.90  1.09   \n",
       "\n",
       "    od280/od315_of_diluted_wines  proline  \n",
       "10                          3.17   1510.0  \n",
       "14                          3.00   1547.0  \n",
       "18                          2.82   1680.0  \n",
       "3                           3.45   1480.0  \n",
       "31                          2.88   1515.0  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.iloc[\n",
    "    nearest_k(\n",
    "        np.array([10.5, 1.2, 13.6, 158.9, 4.2, 3.6, 1.2, \n",
    "                  2.1, 6.2, 1.3, 4.7, 6.9, 1650]),\n",
    "        df_wine.to_numpy(), \n",
    "        5, \n",
    "        lpnorm\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, representing a query text is not as straightforward because each word in the query text must be matched to the correct column position in the database. An elegant solution to this is to create a class that will convert the documents into their BoW representation but will also store the word column positions so that it can transform any text into the same representation afterwards. A word that is not in the original corpus will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "330fb4f2529b6a823a663cdb4ac9ba33",
     "grade": false,
     "grade_id": "cell-4aab87b3fbd950e1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Problem 10**\n",
    "\n",
    "Complete the `Vectorizer` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:06:44.580743Z",
     "start_time": "2020-06-14T02:06:44.567164Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9c712c5b3cccaf066101ebfe102267b",
     "grade": false,
     "grade_id": "cell-bdd6208cddd2da98",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"Class to vectorize\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Inizializes index word and word index\"\"\"\n",
    "        self.index_word = {}\n",
    "        self.word_index = {}\n",
    "        \n",
    "    def build_mappings(self, docs):\n",
    "        \"\"\"Initialize word-index mappings\n",
    "        Parameter\n",
    "        ---------\n",
    "        docs : sequence of str\n",
    "            Corpus to build mappings for\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        \n",
    "        for doc in docs:\n",
    "            words_list = np.unique(np.array(doc.lower().split()))\n",
    "            words.extend(words_list)\n",
    "            \n",
    "        for idx, w in enumerate(np.unique(words)):\n",
    "            self.index_word[idx] = w\n",
    "            self.word_index[w] = idx\n",
    "                                  \n",
    "    def vectorize(self, doc):\n",
    "        \"\"\"Return the BoW vector representation of doc\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        doc : str\n",
    "            Text to compute the vector representation of\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        vec : ndarray\n",
    "            BoW vector representation of doc\n",
    "        \"\"\"\n",
    "        words, wcount = np.unique(np.array(doc.lower().split()),\n",
    "                                  return_counts=True)\n",
    "        vec = np.zeros(len(self.index_word.keys()))\n",
    "        \n",
    "        for idx, word in enumerate(words):\n",
    "            index_w = self.word_index.get(word)\n",
    "            if index_w:\n",
    "                vec[index_w] = wcount[idx]\n",
    "        \n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:06:46.107301Z",
     "start_time": "2020-06-14T02:06:44.583353Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b19f00891b7e7e8915cd5c9d0d385b2",
     "grade": true,
     "grade_id": "cell-2e905c572a3574ae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer()\n",
    "vectorizer.build_mappings(data_newsgroups['data'])\n",
    "vec = vectorizer.vectorize(data_newsgroups['data'][0])\n",
    "assert_array_almost_equal(vec, bow_ng.iloc[0])\n",
    "assert_equal(\n",
    "    vectorizer.vectorize('mary had a little lamb'),\n",
    "    vectorizer.vectorize('mary had a little')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try querying for a document that is not in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T03:11:39.264888Z",
     "start_time": "2020-06-14T03:11:32.437643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>!!</th>\n",
       "      <th>!!!</th>\n",
       "      <th>!!!!</th>\n",
       "      <th>!!!!!!!!!!</th>\n",
       "      <th>!)</th>\n",
       "      <th>!:</th>\n",
       "      <th>!=</th>\n",
       "      <th>!&gt;</th>\n",
       "      <th>!changefsi,</th>\n",
       "      <th>...</th>\n",
       "      <th>~ftp/volvis92</th>\n",
       "      <th>~re00/tmp/sm.2.1.0.tar.z</th>\n",
       "      <th>~~15</th>\n",
       "      <th>~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>Ã¾</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 39080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        !   !!  !!!  !!!!  !!!!!!!!!!   !)   !:   !=   !>  !changefsi,  ...  \\\n",
       "622   0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "847   0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "169   0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "1523  0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "688   0.0  0.0  0.0   0.0         0.0  0.0  0.0  0.0  0.0          0.0  ...   \n",
       "\n",
       "      ~ftp/volvis92  ~re00/tmp/sm.2.1.0.tar.z  ~~15  ~~~~~~~~~~~  \\\n",
       "622             0.0                       0.0   0.0          0.0   \n",
       "847             0.0                       0.0   0.0          0.0   \n",
       "169             0.0                       0.0   0.0          0.0   \n",
       "1523            0.0                       0.0   0.0          0.0   \n",
       "688             0.0                       0.0   0.0          0.0   \n",
       "\n",
       "      ~~~~~~~~~~~~  ~~~~~~~~~~~~~~~~~~~  \\\n",
       "622            0.0                  0.0   \n",
       "847            0.0                  0.0   \n",
       "169            0.0                  0.0   \n",
       "1523           0.0                  0.0   \n",
       "688            0.0                  0.0   \n",
       "\n",
       "      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "622                                                 0.0         \n",
       "847                                                 0.0         \n",
       "169                                                 0.0         \n",
       "1523                                                0.0         \n",
       "688                                                 0.0         \n",
       "\n",
       "      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "622                                                 0.0                         \n",
       "847                                                 0.0                         \n",
       "169                                                 0.0                         \n",
       "1523                                                0.0                         \n",
       "688                                                 0.0                         \n",
       "\n",
       "      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "622                                                 0.0                               \n",
       "847                                                 0.0                               \n",
       "169                                                 0.0                               \n",
       "1523                                                0.0                               \n",
       "688                                                 0.0                               \n",
       "\n",
       "        Ã¾  \n",
       "622   0.0  \n",
       "847   0.0  \n",
       "169   0.0  \n",
       "1523  0.0  \n",
       "688   0.0  \n",
       "\n",
       "[5 rows x 39080 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_ng.iloc[\n",
    "    nearest_k(\n",
    "        vectorizer.vectorize('mary had a little lamb'), \n",
    "        bow_ng.to_numpy(), \n",
    "        5, \n",
    "        dcos)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We can now search for similar objects. In the second part, we will introduce performance measures as well as try to improve the search using normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
